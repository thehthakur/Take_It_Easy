@inproceedings{lattner2004llvm,
  title={LLVM: A compilation framework for lifelong program analysis \& transformation},
  author={Lattner, Chris and Adve, Vikram},
  booktitle={International symposium on code generation and optimization, 2004. CGO 2004.},
  pages={75--86},
  year={2004},
  organization={IEEE}
}

@inproceedings{zhang2021nn,
  title={Nn-meter: Towards accurate latency prediction of deep-learning model inference on diverse edge devices},
  author={Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  booktitle={Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services},
  pages={81--93},
  year={2021}
}

@article{vthakurverified,
author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},
title = {Architecture-Preserving Provable Repair of Deep Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591238},
doi = {10.1145/3591238},
abstract = {Deep neural networks (DNNs) are becoming increasingly important components  
of software, and are considered the state-of-the-art solution for a number  
of problems, such as image recognition. However, DNNs are far from  
infallible, and incorrect behavior of DNNs can have disastrous real-world  
consequences. This paper addresses the problem of architecture-preserving  
V-polytope provable repair of DNNs.  
A V-polytope defines a convex bounded polytope using its vertex representation.  
V-polytope provable repair guarantees that the repaired DNN  
satisfies the given specification on the infinite set of points in the given V-polytope.  
An architecture-preserving repair only modifies the parameters of the DNN, without  
modifying its architecture. The repair has the flexibility to  
modify multiple layers of the DNN, and runs in polynomial time.  
It supports DNNs with activation functions that have some linear pieces,  
as well as fully-connected, convolutional, pooling and residual layers.  
To the best our knowledge, this is the first provable repair approach that  
has all of these features.  
We implement our approach in a tool called APRNN. Using  
MNIST, ImageNet, and ACAS Xu DNNs, we show that  
it has better efficiency, scalability, and generalization  
compared to PRDNN and REASSURE, prior provable repair methods that are  
not architecture preserving.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {124},
numpages = {25},
keywords = {Bug fixing, Deep Neural Networks, Repair, Synthesis}
}

@inproceedings{mirhoseini2017device,
  title={Device placement optimization with reinforcement learning},
  author={Mirhoseini, Azalia and Pham, Hieu and Le, Quoc V and Steiner, Benoit and Larsen, Rasmus and Zhou, Yuefeng and Kumar, Naveen and Norouzi, Mohammad and Bengio, Samy and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={2430--2439},
  year={2017},
  organization={PMLR}
}

@article{jia2019beyond,
  title={Beyond data and model parallelism for deep neural networks.},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={1--13},
  year={2019}
}


@misc{hu2023tpumlircompilertpuusing,
      title={TPU-MLIR: A Compiler For TPU Using MLIR}, 
      author={Pengchao Hu and Man Lu and Lei Wang and Guoyue Jiang},
      year={2023},
      eprint={2210.15016},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2210.15016}, 
}

@article{wess2020annette,
title={Annette: Accurate neural network execution time estimation with stacked models},
  author={Wess, Matthias and Ivanov, Matvey and Unger, Christoph and Nookala, Anvesh and Wendt, Alexander and Jantsch, Axel},
  journal={IEEE Access},
  volume={9},
  pages={3545--3556},
  year={2020},
  publisher={IEEE}
}


@inproceedings{qi2017paleo,
  title={Paleo: A performance model for deep neural networks},
  author={Qi, Hang and Sparks, Evan R and Talwalkar, Ameet},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{osterwind2022hardware,
  title={Hardware Execution Time Prediction for Neural Network Layers},
  author={Osterwind, Adrian and Droste-Rehling, Julian and Vemparala, Manoj-Rohit and Helms, Domenik},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={582--593},
  year={2022},
  organization={Springer}
}

@inproceedings{wang2020perfnet,
  title={Perfnet: Platform-aware performance modeling for deep neural networks},
  author={Wang, Chuan-Chi and Liao, Ying-Chiao and Kao, Ming-Chang and Liang, Wen-Yew and Hung, Shih-Hao},
  booktitle={Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
  pages={90--95},
  year={2020}
}


@book{bishop2023deep,
  title={Deep learning: Foundations and concepts},
  author={Bishop, Christopher M and Bishop, Hugh},
  year={2023},
  publisher={Springer Nature}
}

@inproceedings{wu2024hector,
  title={Hector: An efficient programming and compilation framework for implementing relational graph neural networks in GPU architectures},
  author={Wu, Kun and Hidayeto{\u{g}}lu, Mert and Song, Xiang and Huang, Sitao and Zheng, Da and Nisa, Israt and Hwu, Wen-mei},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={528--544},
  year={2024}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{lattner2021mlir,
  title={MLIR: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

@article{li2020deep,
  title={The deep learning compiler: A comprehensive survey},
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={3},
  pages={708--727},
  year={2020},
  publisher={IEEE}
}

@article{jia2019,
  title={Optimizing DNN computation with relaxed graph substitutions},
  author={Jia, Zhihao and Thomas, James and Warszawski, Todd and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={27--39},
  year={2019}
}

@inproceedings{he2016,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{cormen2009introduction,
  title={Introduction to Algorithms. Thrid Edition},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  journal={London: Massachusetts},
  year={2009}
}

@manual{hloir,
	author= {TensorFlow},
    title={HLO IR, https://github.com/tensorflow/mlir-hlo},
	url={https://github.com/tensorflow/mlir-hlo},
	year={2023}
}


@manual{halideir,
	author= {TVM project},
    title={Halide IR, https://github.com/dmlc/HalideIR},
	url={https://github.com/dmlc/HalideIR},
	year={2023}
}

@manual{pytorchdocs,
	author = {The Linux Foundation},
	title = {PyTorch Docs, https://pytorch.org/docs/stable/index.html},
	url = {https://pytorch.org/docs/stable/index.html},
	year = {2023}
}

@manual{onnxpython,
	author= {The Linux Foundation},
  title        = {ONNX With Python, https://onnx.ai/onnx/intro/python.html},
  url = {https://onnx.ai/onnx/intro/python.html},
  year={2024}

}

@manual{onnxoptimizer,
	author= {The Linux Foundation},
  title        = {ONNX Optimizer, https://github.com/onnx/optimizer},
  url = {https://github.com/onnx/optimizer},
	year={2024}
}

@manual{onnxprofiler,
author= {The Linux Foundation},
title= {ONNX Profiling Tools, https://onnxruntime.ai/docs/performance/tune-performance/profiling-tools.html},
url = {https://onnxruntime.ai/docs/performance/tune-performance/profiling-tools.html},
year={2024}
}

@manual{mbw,
author= {The Linux Foundation},
title= {Memory Bandwidth Benchmarking, https://manpages.ubuntu.com/manpages/focal/man1/mbw.1.html},
url = {https://manpages.ubuntu.com/manpages/focal/man1/mbw.1.html},
year={2019}
}

@manual{openvino,
author= {Intel Corporation},
title= {OpenVINO, https://docs.openvino.ai/2024/index.html},
url = {https://docs.openvino.ai/2024/index.html},
year={2024}
}



@article{click1995,
author = {Click, Cliff and Cooper, Keith D.},
title = {Combining analyses, combining optimizations},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/201059.201061},
doi = {10.1145/201059.201061},
abstract = {Modern optimizing compilers use several passes over a program's intermediate representation to generate good code. Many of these optimizations exhibit a phase-ordering problem. Getting the best code may require iterating optimizations until a fixed point is reached. Combining these phases can lead to the discovery of more facts about the program, exposing more opportunities for optimization. This article presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the frame work provides insight into when a combination yields better results. To make the ideas more concrete, this article presents a framework for combining constant propagation, value numbering, and unreachable-code elimination. It is an open question as to what other frameworks can be combined in this way.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {mar},
pages = {181–196},
numpages = {16},
keywords = {constant propagation, data-flow analysis, optimizing compilers, value numbering}
}

  
@inproceedings{karthik2002,
author = {Gargi, Karthik},
title = {A sparse algorithm for predicated global value numbering},
year = {2002},
isbn = {1581134630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512529.512536},
doi = {10.1145/512529.512536},
abstract = {This paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing global reassociation, it exploits the congruences induced by the predicates of conditional jumps (predicate inference and value inference), and it associates the arguments of acyclic \o{} functions with the predicates controlling their arrival (\o{} predication), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient sparse formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.},
booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation},
pages = {45–56},
numpages = {12},
keywords = {static single assignment, global value numbering},
location = {Berlin, Germany},
series = {PLDI '02}
}

  
 @inproceedings{10.1145/2951913.2976746,
author = {Abadi, Mart\'{\i}n},
title = {TensorFlow: learning functions at scale},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951913.2976746},
doi = {10.1145/2951913.2976746},
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
pages = {1},
numpages = {1},
keywords = {Machine learning, distributed programming},
location = {Nara, Japan},
series = {ICFP 2016}
}

  

@article{tf2016,
author = {Abadi, Mart\'{\i}n},
title = {TensorFlow: learning functions at scale},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022670.2976746},
doi = {10.1145/3022670.2976746},
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {1},
numpages = {1},
keywords = {Machine learning, distributed programming}
}

@misc{pypiOnnx2tf,
	author = {},
	title = {onnx2tf},
	howpublished = {\url{https://pypi.org/project/onnx2tf/}},
	year = {},
	note = {Command-line tool to convert ONNX files to TensorFlow},
}

@misc{pytorchTorchonnxx2014,
	author = {},
	title = {torch.onnx: PyTorch documentation},
	howpublished = {\url{https://pytorch.org/docs/stable/onnx.html}},
	year = {},
}

@misc{githubGitHubOnnxtensorflowonnx,
	author = {},
	title = {{G}it{H}ub - onnx/tensorflow-onnx: {C}onvert {T}ensor{F}low, {K}eras, {T}ensorflow.js and {T}flite models to {O}{N}{N}{X}},
	howpublished = {\url{https://github.com/onnx/tensorflow-onnx}},
	year = {}
}

@misc{onnxONNXOperators,
	author = {},
	title = {{O}{N}{N}{X} {O}perators - {O}{N}{N}{X} 1.18.0 documentation --- onnx.ai},
	howpublished = {\url{https://onnx.ai/onnx/operators/}},
	year = {},
	note = {[Accessed 27-11-2024]},
}


